= Create A Kubernetes Cluster
:toc:
:icons:
:linkattrs:
:imagesdir: ../../resources/images

This section will walk you through how to install a Kubernetes cluster on AWS using EKS.

link:https://aws.amazon.com/eks/[Amazon Elastic Container Service for Kubernetes, window="_blank"] (Amazon EKS) makes it easy to deploy, manage, and scale containerized applications using Kubernetes on AWS.

Amazon EKS runs the Kubernetes management infrastructure for you across multiple AWS availability zones to eliminate a single point of failure. Amazon EKS is certified Kubernetes conformant so you can use existing tooling and plugins from partners and the Kubernetes community. Applications running on any standard Kubernetes environment are fully compatible and can be easily migrated to Amazon EKS.

If you have not set up the link:../../01-container-basics/101-start-here[Cloud9 Development Environment, window="_blank"] yet, please do so before continuing.

== Create a Kubernetes Cluster with EKS

EKS can be used to create a highly available cluster, with multiple master nodes spread across multiple availability zones.

=== Create the master nodes
First set the `EKS_CLUSTER_NAME` environmental variable.  Make sure to provide a unique name.

    $ export EKS_CLUSTER_NAME=<your_unique_cluster_name>

Create a Kubernetes cluster using the following command. Run it in the terminal tab at the bottom of the Cloud9 IDE. This will create a cluster with master nodes:

    $ aws eks create-cluster \
      --name ${EKS_CLUSTER_NAME} \
      --role-arn ${EKS_SERVICE_ROLE} \
      --resources-vpc-config subnetIds=${EKS_SUBNET_IDS},securityGroupIds=${EKS_SECURITY_GROUPS} \
      --kubernetes-version 1.12

The `EKS_SERVICE_ROLE`, `EKS_SUBNET_IDS`, and `EKS_SECURITY_GROUPS` environment variables should have been set during the link:../../01-container-basics/101-start-here[Cloud9 Environment Setup].

Cluster provisioning usually takes less than 10 minutes. You can query the status of your cluster with the following command. When your cluster status is `ACTIVE`, you can proceed.

    $ watch "aws eks describe-cluster --name ${EKS_CLUSTER_NAME} --query cluster.status --output text"

=== Create the configuration file

In order to access the cluster, you must create a Kubectl configuration file (sometimes referred to as a `kubeconfig` file).
This configuration file can be created automatically via an AWS CLI command.

Once the cluster has moved to the `ACTIVE` state, create the Kubectl Configuration file

    $ aws eks update-kubeconfig --name ${EKS_CLUSTER_NAME}

This will create a configuration file at `$HOME/.kube/config` and update the necessary environment variable for default access.

You can test your kubectl configuration using 'kubectl get service'

    $ kubectl get service
    NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
    kubernetes   ClusterIP   10.100.0.1   <none>        443/TCP   8m

=== Create the worker nodes

Now that your EKS master nodes are created, you can launch and configure your worker nodes.

First, set the `EKS_NODE_STACK_NAME` environmental variable.  Make sure the stack name is unique.

    $ export EKS_NODE_STACK_NAME=<your_unique_stack_name>

Next, create an SSH key pair for use with your nodes

    $ ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa && \
    aws ec2 create-key-pair --key-name ${EKS_NODE_STACK_NAME} --query 'KeyMaterial' --output text > $HOME/.ssh/k8s-workshop.pem && /
    chmod 0400 $HOME/.ssh/k8s-workshop.pem

Then launch your worker nodes by running the following CloudFormation CLI command:

    $ aws cloudformation create-stack \
      --stack-name ${EKS_NODE_STACK_NAME} \
      --template-url https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/amazon-eks-nodegroup.yaml \
      --capabilities "CAPABILITY_IAM" \
      --parameters "[{\"ParameterKey\": \"KeyName\", \"ParameterValue\": \"${EKS_NODE_STACK_NAME}\"},
                     {\"ParameterKey\": \"NodeImageId\", \"ParameterValue\": \"${EKS_WORKER_AMI}\"},
                     {\"ParameterKey\": \"ClusterName\", \"ParameterValue\": \"${EKS_CLUSTER_NAME}\"},
                     {\"ParameterKey\": \"NodeGroupName\", \"ParameterValue\": \"${EKS_NODE_STACK_NAME}\"},
                     {\"ParameterKey\": \"ClusterControlPlaneSecurityGroup\", \"ParameterValue\": \"${EKS_SECURITY_GROUPS}\"},
                     {\"ParameterKey\": \"VpcId\", \"ParameterValue\": \"${EKS_VPC_ID}\"},
                     {\"ParameterKey\": \"Subnets\", \"ParameterValue\": \"${EKS_SUBNET_IDS}\"}]"

The `EKS_WORKER_AMI`, `EKS_VPC_ID`, `EKS_SUBNET_IDS`, and `EKS_SECURITY_GROUPS` environment variables should have been set during the build.

Node provisioning usually takes less than 5 minutes. You can query the status of your cluster with the following command. When your cluster status is `CREATE_COMPLETE`, you can proceed.

    $ watch 'aws cloudformation describe-stacks --stack-name "${EKS_NODE_STACK_NAME}" --query "Stacks[0].[StackStatus,Outputs]"'

**Copy the ARN for the NodeInstanceRole from the output and save it for the next step.**

```
[
    "CREATE_COMPLETE",
    [
        {
            "OutputKey": "NodeInstanceRole",
            "OutputValue": "arn:aws:iam::12345678:role/workshop-eks-nodes-NodeInstanceRole-BHCMWDPXZWBP",
            "Description": "The node instance role"
        },
        {
            "OutputKey": "NodeSecurityGroup",
            "OutputValue": "sg-0284c5c671f9e4ac2",
            "Description": "The security group for the node group"
        }
    ]
]
```

=== Enable worker nodes to join cluster

To enable worker nodes to join your cluster, you must apply a ConfigMap to your cluster including the ARN for the IAM role assigned to
the worker nodes.  Under the `~/environment/aws-workshop-for-kubernetes/02-eks-cluster-creation/201-your-first-cluster/templates/` folder, open the `aws-auth-cm.yaml` file with
the Cloud9 file editor. Then update the `rolearn` field so it matches the ARN recorded in the previous step.

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - rolearn: <ARN of instance role (not instance profile)>
      username: system:node:{{EC2PrivateDNSName}}
      groups:
        - system:bootstrappers
        - system:nodes
```

Next, apply this ConfigMap using the Kubectl command

    $ kubectl apply -f ~/environment/aws-workshop-for-kubernetes/02-eks-cluster-creation/201-your-first-cluster/templates/aws-auth-cm.yaml

Watch the status of your nodes and wait for them to reach the `Ready` status.

    $ watch kubectl get nodes
    NAME                            STATUS   ROLES    AGE    VERSION
    ip-172-31-36-248.ec2.internal   Ready    <none>   104s   v1.12.7
    ip-172-31-73-146.ec2.internal   Ready    <none>   103s   v1.12.7
    ip-172-31-80-34.ec2.internal    Ready    <none>   104s   v1.12.7

== Kubernetes Cluster Context

You can manage multiple Kubernetes clusters with _kubectl_, the Kubernetes CLI. We will look more deeply at kubectl in the next section. The configuration for each cluster is stored in a configuration file, referred to as the "`kubeconfig file`". By default, kubectl looks for a file named `config` in the directory `~/.kube`. The kubectl CLI uses kubeconfig file to find the information it needs to choose a cluster and communicate with the API server of a cluster.

This allows you to deploy your applications to different environments by just changing the context. For example, here is a typical flow for application development:

. Build your application using a development environment (perhaps even locally on your laptop)
. Change the context to a test cluster created on AWS
. Use the same command to deploy to the test environment
. Once satisfied, change the context again to a production cluster on AWS
. Once again, use the same command to deploy to production environment

Get a summary of available contexts:

  $ kubectl config get-contexts
  CURRENT   NAME      CLUSTER      AUTHINFO   NAMESPACE
  *         aws       kubernetes   aws

The output shows different contexts, one per cluster, that are available to kubectl. `NAME` column shows the context name. `*` indicates the current context.

View the current context:

  $ kubectl config current-context
  aws

If multiple clusters exist, then you can change the context:

  $ kubectl config use-context <config-name>

You are now ready to continue on with the workshop!

:frame: none
:grid: none
:valign: top


(Optional) If time permits, continue with the next lab link:../202-kubernetes-concepts[to learn about basic Kubernetes Concepts].
